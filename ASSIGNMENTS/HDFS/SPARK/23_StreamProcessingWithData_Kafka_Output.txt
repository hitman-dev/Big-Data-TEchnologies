

23_StreamProcessingWithData.py
---------------------------------------------------------------------------------
OUTPUT:-
---------------------------------------------------------------------------------

2022-01-17 17:08:50,549 INFO v2.WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@3666b03d is committing.
-------------------------------------------
Batch: 1
-------------------------------------------
2022-01-17 17:08:50,610 INFO codegen.CodeGenerator: Code generated in 14.424775 ms
2022-01-17 17:08:50,664 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.106:33469 in memory (size: 1969.0 B, free: 434.4 MiB)
2022-01-17 17:08:50,684 INFO codegen.CodeGenerator: Code generated in 21.968369 ms
+----+--------------------+
| key|               value|
+----+--------------------+
|key1|1000|1000|1010|90...|
+----+--------------------+

2022-01-17 17:08:50,697 INFO v2.WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@3666b03d committed.
2022-01-17 17:08:50,716 INFO streaming.CheckpointFileManager: Writing atomically to hdfs://127.0.0.1:9000/tmp/temporary-9c6a0d5d-4eb0-4cbb-a0c3-450a054edb11/commits/1 using temp file hdfs://127.0.0.1:9000/tmp/temporary-9c6a0d5d-4eb0-4cbb-a0c3-450a054edb11/commits/.1.a17c61f6-ef72-4fdf-9034-adb389c04bb9.tmp
2022-01-17 17:08:50,721 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.106:33469 in memory (size: 4.4 KiB, free: 434.4 MiB)
2022-01-17 17:08:51,236 INFO streaming.CheckpointFileManager: Renamed temp file hdfs://127.0.0.1:9000/tmp/temporary-9c6a0d5d-4eb0-4cbb-a0c3-450a054edb11/commits/.1.a17c61f6-ef72-4fdf-9034-adb389c04bb9.tmp to hdfs://127.0.0.1:9000/tmp/temporary-9c6a0d5d-4eb0-4cbb-a0c3-450a054edb11/commits/1
2022-01-17 17:08:51,244 INFO streaming.MicroBatchExecution: Streaming query made progress: {
  "id" : "b12a686b-43ef-47c5-b8c0-d9a11263e305",
  "runId" : "9c56a232-0737-483e-9cbb-12e01179052d",
  "name" : null,
  "timestamp" : "2022-01-17T11:38:49.212Z",
  "batchId" : 1,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 66.66666666666667,
  "processedRowsPerSecond" : 0.4938271604938272,
  "durationMs" : {
    "addBatch" : 1290,
    "getBatch" : 1,
    "latestOffset" : 4,
    "queryPlanning" : 81,
    "triggerExecution" : 2024,
    "walCommit" : 108
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[train_topic]]",
    "startOffset" : {
      "train_topic" : {
        "0" : 5
      }
    },
    "endOffset" : {
      "train_topic" : {
        "0" : 6
      }
    },
    "latestOffset" : {
      "train_topic" : {
        "0" : 6
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 66.66666666666667,
    "processedRowsPerSecond" : 0.4938271604938272,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11461fee",
    "numOutputRows" : 1
  }
}
2022-01-17 17:08:51,246 INFO internals.SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-89e24b46-bf40-4160-942e-145f43fdee9e-1890977847-driver-0-1, groupId=spark-kafka-source-89e24b46-bf40-4160-942e-145f43fdee9e-1890977847-driver-0] Seeking to LATEST offset of partition train_topic-0
