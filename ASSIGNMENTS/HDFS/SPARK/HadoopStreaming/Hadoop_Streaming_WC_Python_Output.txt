

Hadoop_Streaming
---------------------------------------------------------------------------------
OUTPUT:-
---------------------------------------------------------------------------------

hitman@ubuntu:~/DBDA_HOME/hadoop-3.3.1$ cp ~/DBDA_HOME/DBDA_CODE/SPARK/PYTHON/HadoopStreaming/*.py . 
hitman@ubuntu:~/DBDA_HOME/hadoop-3.3.1$ cp ~/DBDA_HOME/DBDA_CODE/SPARK/PYTHON/HadoopStreaming/*.sh .
hitman@ubuntu:~/DBDA_HOME/hadoop-3.3.1$ ls
bin      lib             licenses-binary  mapper.py      README.txt  sbin
etc      libexec         LICENSE.txt      NOTICE-binary  reducer.py  share
include  LICENSE-binary  logs             NOTICE.txt     runner.sh
hitman@ubuntu:~/DBDA_HOME/hadoop-3.3.1$ cat runner.sh 
hadoop jar ./share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar -input /LICENSE.txt -output /stream_out_2 -mapper mapper.py -reducer reducer.py -file mapper.py -file reducer.py
----------------------------------------------------------------------------------
# Chanding input file to custom input file
----------------------------------------------------------------------------------
hitman@ubuntu:~/DBDA_HOME/hadoop-3.3.1$ hadoop jar ./share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar -input /marvel.txt -output /stream_out_2 -mapper mapper.py -reducer reducer.py -file mapper.py -file reducer.py 
__________________________________________________________________________________
# Output of above command
----------------------------------------------------------------------------------
2022-01-17 18:04:58,168 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.
packageJobJar: [mapper.py, reducer.py, /tmp/hadoop-unjar2132467699939336904/] [] /tmp/streamjob4766414903375318956.jar tmpDir=null
2022-01-17 18:04:59,352 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032
2022-01-17 18:04:59,525 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032
2022-01-17 18:05:01,304 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hitman/.staging/job_1642422450557_0004
2022-01-17 18:05:02,859 INFO mapred.FileInputFormat: Total input files to process : 1
2022-01-17 18:05:03,376 INFO mapreduce.JobSubmitter: number of splits:2
2022-01-17 18:05:05,260 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1642422450557_0004
2022-01-17 18:05:05,260 INFO mapreduce.JobSubmitter: Executing with tokens: []
2022-01-17 18:05:06,689 INFO conf.Configuration: resource-types.xml not found
2022-01-17 18:05:06,689 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2022-01-17 18:05:07,870 INFO impl.YarnClientImpl: Submitted application application_1642422450557_0004
2022-01-17 18:05:08,018 INFO mapreduce.Job: The url to track the job: http://ubuntu:8088/proxy/application_1642422450557_0004/
2022-01-17 18:05:08,020 INFO mapreduce.Job: Running job: job_1642422450557_0004
2022-01-17 18:05:24,393 INFO mapreduce.Job: Job job_1642422450557_0004 running in uber mode : false
2022-01-17 18:05:24,397 INFO mapreduce.Job:  map 0% reduce 0%
2022-01-17 18:05:33,722 INFO mapreduce.Job:  map 100% reduce 0%
2022-01-17 18:05:40,794 INFO mapreduce.Job:  map 100% reduce 100%
2022-01-17 18:05:41,831 INFO mapreduce.Job: Job job_1642422450557_0004 completed successfully
2022-01-17 18:05:41,970 INFO mapreduce.Job: Counters: 54
        File System Counters
                FILE: Number of bytes read=1149
                FILE: Number of bytes written=830687
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=1104
                HDFS: Number of bytes written=736
                HDFS: Number of read operations=11
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=2
                HDFS: Number of bytes read erasure-coded=0
        Job Counters 
                Launched map tasks=2
                Launched reduce tasks=1
                Data-local map tasks=2
                Total time spent by all maps in occupied slots (ms)=13829
                Total time spent by all reduces in occupied slots (ms)=3935
                Total time spent by all map tasks (ms)=13829
                Total time spent by all reduce tasks (ms)=3935
                Total vcore-milliseconds taken by all map tasks=13829
                Total vcore-milliseconds taken by all reduce tasks=3935
                Total megabyte-milliseconds taken by all map tasks=14160896
                Total megabyte-milliseconds taken by all reduce tasks=4029440
        Map-Reduce Framework
                Map input records=1
                Map output records=104
                Map output bytes=935
                Map output materialized bytes=1155
                Input split bytes=168
                Combine input records=0
                Combine output records=0
                Reduce input groups=70
                Reduce shuffle bytes=1155
                Reduce input records=104
                Reduce output records=70
                Spilled Records=208
                Shuffled Maps =2
                Failed Shuffles=0
                Merged Map outputs=2
                GC time elapsed (ms)=177
                CPU time spent (ms)=3350
                Physical memory (bytes) snapshot=801316864
                Virtual memory (bytes) snapshot=8286461952
                Total committed heap usage (bytes)=679477248
                Peak Map Physical memory (bytes)=317222912
                Peak Map Virtual memory (bytes)=2761146368
                Peak Reduce Physical memory (bytes)=217628672
                Peak Reduce Virtual memory (bytes)=2775863296
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters 
                Bytes Read=936
        File Output Format Counters 
                Bytes Written=736
2022-01-17 18:05:41,970 INFO streaming.StreamJob: Output directory: /stream_out_2

----------------------------------------------------------------------------------
hitman@ubuntu:~/DBDA_HOME/hadoop-3.3.1$ hdfs dfs -cat /stream_out_2/part-00000
----------------------------------------------------------------------------------
# Output of above command
----------------------------------------------------------------------------------
$25      1
(MCU)    1
12       1
2007,    1
27       1
American         1
Avengers:        1
Cinematic        1
Comics.          1
Endgame,         1
It       1
MCU      1
Marvel   4
Studios          2
The      3
This     1
Universe         1
a        1
all      3
and      2
appear   1
are      2
at       2
based    1
became   1
been     1
billion          1
box      1
by       2
characters       1
development.     1
film     2
films    4
films,   1
franchise        1
global   1
grossed          1
has      1
have     1
having   1
highest-grossing         2
in       5
includes         1
is       2
least    1
more     1
of       5
office.          1
on       1
over     1
produced         2
production       1
publications     1
release.         1
released         1
series   1
set.     1
shared   1
since    1
stages   1
superhero        1
that     2
the      5
time     2
time,    1
universe         1
upon     1
various          1
which    2
with     1
hitman@ubuntu:~/DBDA_HOME/hadoop-3.3.1$
